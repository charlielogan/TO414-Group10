---
title: 'TO414 Project 3: Spotify'
author: "Ankita Mohapatra"
date: "4/23/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
# Objective

Data: In this report, we are working with a Spotify dataset, which contains audio features of around 600k songs released in between 1922 and 2021.

Intended Audience: Music producers, artists, advertisers
Overarching Objective: Predict Danceability, Predict Popularity, Predict if a Song is from a Particular Decade (80â€™s)
Underlying Objective 1 (for music producers and artists): Recognize which aspects correlate to popularity and creating mega hits
Underlying Objective 2 (for Spotify): Help create automatic classification in recommendation algorithms and creating playlists (based on decade for example)
Underlying Objective 3 (for advertisers): Recognize what kind of music works best 

1. Descriptive Analytics: How do audio features differ across decades? How do audio features differ by popularity? 
2. Predictive Analytics: Can we predict danceability? Can we predict popularity? Can we predict if a song is from a particular decade?

Finally, we will identify several applications for the insights presented in this analysis.

# Data Preparation

## Pull & View Main Data
```{r}
set.seed(1234)
spotify <- read.csv("data.csv")
#spotify_by_artist <- read.csv("data_by_artist.csv")
str(spotify)
#summary(spotify)

audio_features <- read.csv("tracks.csv")
```
### Libraries
```{r}
packages <- c("dplyr", "class", "caret", "gmodels", "C50")
lapply(packages, require, character.only = TRUE)
```

### Data Manipulation
Main Data
```{r}
spotify <- subset(spotify, select = -c(id, name, release_date))
spotify$artists <- as.factor(spotify$artists)
spotify$key <- as.factor(spotify$key)
```

Spotify Main Set
```{r}
# Cut year into bins by decade
spotify$decades <- cut(spotify$year, breaks = c(0,1929, 1939, 1949, 1959, 1969, 1979, 1989, 1999, 2009, 2019, Inf), labels = c("20s", "30s", "40s", "50s", "60s", "70s", "80s", "90s", "2000s", "2010s", "2020s"))


# Classify popularity into bins by 75% and 90% percentiles
popularity_threshold <- 42
quantile(spotify$popularity, probs=seq(.1, .9, by = .1))
spotify$hit_class <- ifelse(spotify$popularity >= 56, "mega-hit", ifelse(spotify$popularity > popularity_threshold, "hit", "not hit"))
spotify$hit_class <- as.factor(spotify$hit_class)
# true or false if top 10% or not
spotify$binary_hit_class <- ifelse(spotify$hit_class == "mega-hit", 1, 0)
spotify$binary_hit_class <- as.factor(spotify$binary_hit_class)
table(spotify$hit_class)

```

Audio Features
```{r}
audio_features$release_year <- substr(audio_features$release_date,1,4)
audio_features$release_year <- as.numeric(audio_features$release_year)
audio_features$key <- as.factor(audio_features$key)
boxplot(audio_features$release_year, horizontal = T, main = "Year of Song Release")
audio_features <- subset(audio_features, select = -c(id, name, popularity, artists, id_artists, release_date))
```
```{r}
audio_features <- audio_features %>%
  select(danceability,everything())   #bring `danceability` to first column
```

## Normalized Sets
Normalize method
```{r}
normalize <- function(x){
  return ((x-min(x))/(max(x)-min(x)))
}

```

### Spotify set

### Audio Features set
```{r}
audio_featuresMM <- as.data.frame(model.matrix(~.-1,audio_features))  #all columns are numeric
audio_featuresN<- lapply(audio_featuresMM,normalize) #apply normalize() to all columns
audio_featuresN <- as.data.frame(audio_featuresN)
```

## Test & Train
Un-normalized Train & Test Sets
```{r}
test_set <- sample(1:nrow(spotify), 30000)
spotify_train <- spotify[-test_set, ]
spotify_test <- spotify[test_set, ]

```

Normalized Train & Test Sets
```{r}
#audio_features
audio_train <- audio_featuresN[-test_set,]
audio_test <- audio_featuresN[test_set,]

#Response / Labels
audio_train_labs <- audio_featuresN[-test_set,"danceability"]
audio_test_labs <- audio_featuresN[test_set,"danceability"]
```


# General Music Attributes
## By Decade
First, we wanted to see what kind of music is most popular on spotify.

```{r}
ggplot(spotify, aes(x = decades, y = popularity, fill=decades)) + 
  geom_bar(stat = "summary", fun = "mean")
```

The most listened to songs are from the 90s and 2000s. This makes sense since that is the generation that started being comfortable with new technology, and also that music has been on the platform longer than the music from 2010s or 2020s. We predict that people that listen to older music might listen to it in different forms (i.e. CDs, records, tapes, etc.). Next, we wanted to look at the different audio features over the decades.

```{r}
ggplot(spotify, aes(x = decades, y = tempo, fill=decades)) + 
  geom_bar(stat = "summary", fun = "mean")
ggplot(spotify, aes(x = decades, y = danceability, fill=decades)) + 
  geom_bar(stat = "summary", fun = "mean")
```
There is a trend in these graphs where the features start out at a certain point in the 20s and then drop down in the 30s-50s, and then increase rather steadily. Our team theorized that this may be due to the Great Depression and prohibition happening at the beginning of the 30s that caused this turn in culture.

```{r}
ggplot(spotify, aes(x = decades, y = explicit, fill=decades)) + 
  geom_bar(stat = "summary", fun = "mean")
```
We were surprised at the graph showing the trends in explicitness because they varied so much and the average seems to be really high for the 20s. We think this is because there is a much smaller number of songs from the 20s in this data set. It dropped off to almost 0 in the 60s and 70s, and then shot up with the prevalence of rock and hip hop.
```{r}
ggplot(spotify, aes(x = decades, y = loudness, fill=decades)) + 
  geom_bar(stat = "summary", fun = "mean")
```
Finally, we looked at the loudness. 
```{r}
summary(spotify)
quantile(spotify$popularity, probs=seq(.1, .9, by = .1))
spotify$hit_class <- ifelse(spotify$popularity >= 56, "mega-hit", ifelse(spotify$popularity > 42, "hit", "not hit"))
spotify$hit_class <- as.factor(spotify$hit_class)
```

## By Popularity
Next, we wanted to see how the audio features differ across our three classes of popularity: not a hit, hit, and mega hit.
```{r}
ggplot(spotify, aes(x = hit_class, y = danceability, fill=hit_class)) + 
  geom_bar(stat = "summary", fun = "mean")+ggplot(spotify, aes(x = hit_class, y = energy, fill=hit_class)) + 
  geom_bar(stat = "summary", fun = "mean")

ggplot(spotify, aes(x = hit_class, y = explicit, fill=hit_class)) + 
  geom_bar(stat = "summary", fun = "mean")+ggplot(spotify, aes(x = hit_class, y = tempo, fill=hit_class)) + 
  geom_bar(stat = "summary", fun = "mean")
```

In the previous graphs, we see that mega hit has the highest mean for danceability, energy, explicitness, and tempo. While there are other factors that determine popularity, such as artist recognition, we can see that high values in these factors is correlated with popularity.

In the following graphs, we see a similar trend but in the opposite direction. Mega hits have the lowest averages for acousticness, instrumentalness, liveness, mode, loudness, and valence. 

```{r}
ggplot(spotify, aes(x = hit_class, y = acousticness, fill=hit_class)) + 
  geom_bar(stat = "summary", fun = "mean")+ ggplot(spotify, aes(x = hit_class, y = instrumentalness, fill=hit_class)) + 
  geom_bar(stat = "summary", fun = "mean")

ggplot(spotify, aes(x = hit_class, y = liveness, fill=hit_class)) + 
  geom_bar(stat = "summary", fun = "mean")+ggplot(spotify, aes(x = hit_class, y = mode, fill=hit_class)) + 
  geom_bar(stat = "summary", fun = "mean")

ggplot(spotify, aes(x = hit_class, y = loudness, fill=hit_class)) + 
  geom_bar(stat = "summary", fun = "mean")+ggplot(spotify, aes(x = hit_class, y = valence, fill=hit_class)) + 
  geom_bar(stat = "summary", fun = "mean")
```

Finally, we also wanted to look at the artists with the most songs in the dataset.
```{r}
sort(table(spotify$artists), decreasing = TRUE)[1:20]
```


# Decade Prediction
## Is the song from the 80s?
```{r}
eighties_train <- subset(spotify_train, select = -c(artists, year, hit_class))
eighties_train$eighties <- ifelse(eighties_train$decades == "80s", 1, 0)
eighties_test$eighties <- ifelse(eighties_test$decades == "80s", 1, 0)
```
First We prepared the train and test data for the 80s prediction test. We removed certain columns from the data set that were irrelevant or dependent on a songs release decade such as artists, year, and hit_class. We then added a boolean column to the data sets indicating whether the song was released in the 1980s or not.


```{r}

eighties_log <- glm(eighties ~ acousticness + danceability + duration_ms + energy + explicit + 
                      instrumentalness + liveness + loudness + mode + 
                      speechiness + tempo + key + instrumentalness*danceability + 
                      duration_ms*acousticness + duration_ms*valence + duration_ms*valence + danceability*instrumentalness, 
                    data = eighties_train, family = "binomial")
summary(eighties_log)

eighties_pred_log <- predict(eighties_log, eighties_test, type = "response")
eighties_pred_log <- ifelse(eighties_pred_log > .23, 1, 0)
summary(eighties_pred_log)
confusionMatrix(as.factor(eighties_pred_log), as.factor(eighties_test$eighties))
```
First we constructed a Logistic Regression to predict whether a song was released in the 80s. The regression resulted in an accuracy of 83.93% and a Kappa value of .2623. The model has many false negatives and false positives and doesn't seem to do a great job distinguishing between 80s music and other decades. Our model suggests that 80s songs have less acousticness, more instrumentals, are more danceable, and have high energy. They also tend to have a faster tempo.

One qualifying factor is that music progression and change isn't discrete even though we are trying to classify music into discrete buckets. For example, a song released in 1979 is likely to sound like a song released in 1980 despite being from a different decade. Additionally, I made the threshold for the log odds value needed to be classified as a 80s song .23 because it maximized the Kappa value for the model. The lower threshold value makes sense because any given song is much more likely to not be from the 80s, so our model has trouble indicating with higher certainty that a song is from the 80s.

```{r}

library(C50)
error_costs = matrix(c(0, 1, 2, 0), nrow = 2)
eighties_dtm <- C5.0(as.factor(eighties) ~ acousticness + danceability + duration_ms + energy + explicit + 
                      instrumentalness + liveness + loudness + mode + 
                      speechiness + tempo + key, data = eighties_train, costs = error_costs)


eighties_pred_dt <- predict(eighties_dtm, eighties_test)
confusionMatrix(as.factor(eighties_pred_dt), as.factor(eighties_test$eighties))

```
Our decision tree model performed slightly better than our logistc regression model. The decision tree has an accuracy of 85.69% and a Kappa value of .2643. Because there are many more non 80s songs than 80s, I added a cost matrix to encouraget the decision tree to minimize false negative responses.

## Which decades are easiest to predict?
```{r}
decade_dtm <- C5.0(decades ~ acousticness + danceability + duration_ms + energy + explicit + 
                      instrumentalness + liveness + loudness + 
                      speechiness + tempo + valence, data = spotify_train)

decades_pred <- predict(decade_dtm, spotify_test, type = "class")
summary(decades_pred)
confusionMatrix(decades_pred, as.factor(spotify_test$decades))
```
The decision tree performed decently well considering how many classes there are and the difficulties in predcting a song's release decade. The model has an accuracy of 34.75% and a Kappa value of .2724. Its interesting to look along the diagnol in the confusion matrix and see that for any given decade of music, the decision tree model predicted the correct decade more than any other individual decade. This indicated that the model is catching onto the trends and differences in music by decade. 

It's also interesting to look at how the decision tree is more likely to mistake a songs decade with an adjacent decade than one further away in time. 
```{r}
fifties_preds <- c(124, 298, 598, 1121, 579, 236, 131, 153, 63, 153, 19)

barplot(fifties_preds, main="Decision Tree Predictions for Songs made in the 50s",
        xlab="Predicted Decade", names.arg=c("20s", "30s", "40s", "50s", "60s", "70s", "80s", "90s", "2000s", "2010s", "2020s"))
```

For example, for all 50s songs, our model accurately predicted 1121 of them. One decade away from the 50s is the 40s and 60s, which recieved 598 and 579 predictions respectively. The general trend is the further away in time a decade is from a songs true decade, the less likely our model is to incorrectly classify it into that decade. this relates back to my comments above about change in music not being discrete but continuous. Songs from the 50s and 60s will sound  more alike than songs from the 50s and 90s, and our model is picking up on that. There are many similarities between the musical decades, and music waves and changes don't necessarily line up with the end of a decade and the start of a new one. That's why its interestign to see and interpret our models results in a histogram rather than just as a binary correct or incorrect prediction.

# Danceability Prediction
Spotify defines danceability as follows:
``Danceability describes how suitable a track is for dancing based on a combination of musical elements including tempo, rhythm stability, beat strength, and overall regularity.
A value of 0.0 is least danceable and 1.0 is most danceable.''

(This was the first model we ran together)
```{r}
dance_model <- lm(danceability ~ tempo + valence + speechiness + 
                    mode + liveness + key + instrumentalness + 
                    explicit + energy + duration_ms + acousticness, data = spotify)
summary(dance_model)
```

By running regression models on the audio features dataset, we can try to predict to what extent the danceability algorithm incorporates the various elements such as those mentioned in the definition above.

```{r}
dm1 <- lm(danceability ~ ., data = audio_train)
```

# Popularity Prediction
## What Makes a Song Popular?
Though the artist or record label may be a driving factor in the popularity of a song, predicting popularity based on song attributes alone could be very valuable information for an artist, someone who works for an artist, or a music streaming service. To find the tendencies of the most popular songs and try to see if we can classify them, we broke the data set into 2 classes: "mega-hit", a song in the top 10% of popularity on Spotify, and all other songs. Popularity of songs is never guaranteed even with multi-million dollar marketing budgets for popular artists, so any insight into what interior attributes make a song popular can be a strategic advantage. 

### Log Model for Hit Classification (mega-hit vs not) 
We can start by training a logistic regression model to classify songs as mega-hits vs non-mega-hits.
```{r, cache=TRUE}
log_test_labels <- spotify_test$binary_hit_class
log_test <- subset(spotify_test, select=-c(binary_hit_class))
log_train <- subset(spotify_train, select=-c(artists, popularity, year, hit_class))
log_model <- train(binary_hit_class ~ ., data = log_train, method = "glm", family="binomial")
summary(log_model)
```
The coefficients and p-values tell us that songs that are highly danceable, not too long and have lower energy, speechiness and liveness. We can use predict and the model generated by the train fucntion to make a classification for our test set.
#### Evaluate Log Model
```{r}
pred <- predict(log_model, log_test)
confusionMatrix(pred, log_test_labels)
```
The logistic model attained an overall accuracy of ~91%, though given the vast majority of the elements in the test set were not hits, the overall accuracy is less descriptive than we'd like. Though classifying a song as not a mega-hit is still useful, our main prediction of interest is to find which songs will be mega-hits. Of the 1057 songs that our model classified as a mega-hit, it successfully classified 721 of them (~68%).
### KNN
Another good option specifically for classifying human behavioral tendencies (popular culture in this case) is a K-nearest neighbors algorithm. To predict a new song's popularity, the algorithm will search for songs with similar attributes and make a classification based on the popularity of those nearby songs. We normalize this data to be on a 0 to 1 scale so as not to overemphasize inputs with larger magnitude. 
```{r}
knn_train <- subset(log_train, select=-c(decades, key))
knn_test <- subset(log_test, select=-c(artists, popularity, year, decades, hit_class, key))
knn_train <- as.data.frame(lapply(knn_train[1:(ncol(knn_train)-1)], normalize))
knn_test <- as.data.frame(lapply(knn_test[1:ncol(knn_test)], normalize))
knn_model_pred <- knn(train=knn_train, test=knn_test, cl=log_train$binary_hit_class, k=95)
```

#### Evaluate KNN Model
```{r}
confusionMatrix(knn_model_pred, log_test_labels)
```

The KNN model also faired decently well in classifying "mega-hit" songs, having a slightly lower accuracy than the log model at ~90%. Additionally the KNN was less willing to make a mega-hit classification, which is reasonable due to the significantly higher amount of non hits in the data set. The KNN still correctly classified 458/692 "mega-hit" classifications, about 66%. The model improved slightly by dropping K, the threshold for making a classification, to 1/4 of the square root of input rows, which is the typical starting point for a K value. Overall, the logistic model was more successful in completing the task we set out to accomplish in building these models: correctly predict which songs have the potential to become extremely popular. 
## Holding artist constant, what makes a song popular?

# Conclusion
