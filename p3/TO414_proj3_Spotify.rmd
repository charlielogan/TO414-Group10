---
title: 'TO414 Project 3: Spotify'
author: "Ankita Mohapatra"
date: "4/23/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
# Objective

Data: In this report, we are working with a Spotify dataset, which contains audio features of around 600k songs released in between 1922 and 2021.

Intended Audience: Music producers, artists, advertisers
Overarching Objective: Our goal is to better understand music consumption patterns and trends among popular songs through the years.
Underlying Objective 1 (for music producers and artists): Recognize which aspects correlate to popularity and creating mega hits
Underlying Objective 2 (for Spotify): Help create automatic classification in recommendation algorithms and creating playlists (based on decade for example).
Underlying Objective 3 (for advertisers): Recognize what kind of music is most popular for advertisers to target intended audiences. 

1. Descriptive Analytics: How do audio features differ across decades? How do audio features differ by popularity? 
2. Predictive Analytics: Can we predict danceability? Can we predict popularity? Can we predict if a song is from a particular decade? Can we find patterns among a single artist's most popular songs?

Finally, we will identify several applications for the insights presented in this analysis.

## Load Spotify Data
```{r}
#set random seed
set.seed(1234)
#load csvs
spotify <- read.csv("data.csv")
audio_features <- read.csv("tracks.csv")
```
## Glossary of Varaibles
The spotify and audio features datasets contain a combination of objective and subjective musical attributes.

Objective attributes of a musical track which are available in these datasets include:

* `key` the major key of the track, where each number corresponds to a unique key
* `loudness` average volume of the track in decibels
* `mode` major or minor classification
* `duration` time duration of the track in milliseconds
* `release_year` of the album containing this track
* `tempo` of the track in beats per minute (BPM)

Subjective attributes of a musical track which are available in these datasets, calculated in function of other measured properties, include:

* `danceability` the track's suitability for dancing, on a 0-1 scale
* `energy` how energetic the track is, on a 0-1 scale
* `valence` how positive/cheerful the music is, on a 0-1 scale
* `acousticness` how acoustic the song is, on a 0-1 scale

Finally, there are some variables which have used a [likely subjective] algorithm to describe objective attributes, resulting in a debatable classification:

* `liveness` presence of audience, audible in the track, on a 0-1 scale
* `instrumentalness` ratio of instrumental sounds throughout track, on a 0-1 scale
* `speechiness` ratio of spoken words
* `time_signature` predicted time signature of the track

## Load Libraries
```{r}
#load libraries
packages <- c("dplyr", "class", "caret", "gmodels", "C50", "patchwork")
lapply(packages, require, character.only = TRUE)
```

## Data Manipulation
Main Data
```{r}
spotify <- subset(spotify, select = -c(id, name, release_date))
spotify$artists <- as.factor(spotify$artists)
spotify$key <- as.factor(spotify$key)

# Cut year into bins by decade
spotify$decades <- cut(spotify$year, breaks = c(0,1929, 1939, 1949, 1959, 1969, 1979, 1989, 1999, 2009, 2019, Inf), labels = c("20s", "30s", "40s", "50s", "60s", "70s", "80s", "90s", "2000s", "2010s", "2020s"))


# Classify popularity into bins by 75% and 90% percentiles
popularity_threshold <- 42
quantile(spotify$popularity, probs=seq(.1, .9, by = .1))
spotify$hit_class <- ifelse(spotify$popularity >= 56, "mega-hit", ifelse(spotify$popularity > popularity_threshold, "hit", "not hit"))
spotify$hit_class <- as.factor(spotify$hit_class)
# true or false if top 10% or not
spotify$binary_hit_class <- ifelse(spotify$hit_class == "mega-hit", 1, 0)
spotify$binary_hit_class <- as.factor(spotify$binary_hit_class)
table(spotify$hit_class)

```

Audio Features
```{r}
audio_features$release_year <- substr(audio_features$release_date,1,4)
audio_features$release_year <- as.numeric(audio_features$release_year)
audio_features$key <- as.factor(audio_features$key)
boxplot(audio_features$release_year, horizontal = T, main = "Year of Song Release")
audio_features <- subset(audio_features, select = -c(id, name, popularity, artists, id_artists, release_date))
```
```{r}
audio_features <- audio_features %>%
  select(danceability,everything())   #bring `danceability` to first column
```

## Test & Train
Split spotify set into test and train sets to be used throughout
```{r}
test_set <- sample(1:nrow(spotify), 30000)
spotify_train <- spotify[-test_set, ]
spotify_test <- spotify[test_set, ]
```


# General Music Attributes
## By Decade
First, we wanted to see what kind of music is most popular on spotify.

```{r}
ggplot(spotify, aes(x = decades, y = popularity, fill=decades)) + 
  geom_bar(stat = "summary", fun = "mean")
```

The most listened to songs are from the 90s and 2000s. This makes sense since that is the generation that started being comfortable with new technology, and also that music has been on the platform longer than the music from 2010s or 2020s. We predict that people that listen to older music might listen to it in different forms (i.e. CDs, records, tapes, etc.). Next, we wanted to look at the different audio features over the decades.

```{r}
ggplot(spotify, aes(x = decades, y = tempo, fill=decades)) + 
  geom_bar(stat = "summary", fun = "mean")
ggplot(spotify, aes(x = decades, y = danceability, fill=decades)) + 
  geom_bar(stat = "summary", fun = "mean")
```
There is a trend in these graphs where the features start out at a certain point in the 20s and then drop down in the 30s-50s, and then increase rather steadily. Our team theorized that this may be due to the Great Depression and prohibition happening at the beginning of the 30s that caused this turn in culture.

```{r}
ggplot(spotify, aes(x = decades, y = explicit, fill=decades)) + 
  geom_bar(stat = "summary", fun = "mean")
```
We were surprised at the graph showing the trends in explicitness because they varied so much and the average seems to be really high for the 20s. We think this is because there is a much smaller number of songs from the 20s in this data set. It dropped off to almost 0 in the 60s and 70s, and then shot up with the prevalence of rock and hip hop.
```{r}
ggplot(spotify, aes(x = decades, y = loudness, fill=decades)) + 
  geom_bar(stat = "summary", fun = "mean")
```
Finally, we looked at the loudness. 
```{r}
summary(spotify)
quantile(spotify$popularity, probs=seq(.1, .9, by = .1))
spotify$hit_class <- ifelse(spotify$popularity >= 56, "mega-hit", ifelse(spotify$popularity > 42, "hit", "not hit"))
spotify$hit_class <- as.factor(spotify$hit_class)
```

## By Popularity
Next, we wanted to see how the audio features differ across our three classes of popularity: not a hit, hit, and mega hit.
```{r}
ggplot(spotify, aes(x = hit_class, y = danceability, fill=hit_class)) + 
  geom_bar(stat = "summary", fun = "mean")+ggplot(spotify, aes(x = hit_class, y = energy, fill=hit_class)) + 
  geom_bar(stat = "summary", fun = "mean")+ggplot(spotify, aes(x = hit_class, y = explicit, fill=hit_class)) + 
  geom_bar(stat = "summary", fun = "mean")+ggplot(spotify, aes(x = hit_class, y = tempo, fill=hit_class)) + 
  geom_bar(stat = "summary", fun = "mean")
```

In the previous graphs, we see that mega hit has the highest mean for danceability, energy, explicitness, and tempo. While there are other factors that determine popularity, such as artist recognition, we can see that high values in these factors is correlated with popularity.

In the following graphs, we see a similar trend but in the opposite direction. Mega hits have the lowest averages for acousticness, instrumentalness, liveness, mode, loudness, and valence. 

```{r}
ggplot(spotify, aes(x = hit_class, y = acousticness, fill=hit_class)) + 
  geom_bar(stat = "summary", fun = "mean")+ ggplot(spotify, aes(x = hit_class, y = instrumentalness, fill=hit_class)) + 
  geom_bar(stat = "summary", fun = "mean")+ggplot(spotify, aes(x = hit_class, y = liveness, fill=hit_class)) + 
  geom_bar(stat = "summary", fun = "mean")+ggplot(spotify, aes(x = hit_class, y = mode, fill=hit_class)) + 
  geom_bar(stat = "summary", fun = "mean")

ggplot(spotify, aes(x = hit_class, y = loudness, fill=hit_class)) + 
  geom_bar(stat = "summary", fun = "mean")+ggplot(spotify, aes(x = hit_class, y = valence, fill=hit_class)) + 
  geom_bar(stat = "summary", fun = "mean")
```

Finally, we also wanted to look at the artists with the most songs in the dataset.
```{r}
sort(table(spotify$artists), decreasing = TRUE)[1:20]
```


# Decade Prediction
## Is the song from the 80s?
```{r}
eighties_train <- subset(spotify_train, select = -c(artists, year, hit_class))
eighties_test <- subset(spotify_test, select = -c(artists, year, hit_class))
eighties_train$eighties <- ifelse(eighties_train$decades == "80s", 1, 0)
eighties_test$eighties <- ifelse(eighties_test$decades == "80s", 1, 0)
```
First We prepared the train and test data for the 80s prediction test. We removed certain columns from the data set that were irrelevant or dependent on a songs release decade such as artists, year, and hit_class. We then added a boolean column to the data sets indicating whether the song was released in the 1980s or not.


```{r}

eighties_log <- glm(eighties ~ acousticness + danceability + duration_ms + energy + explicit + 
                      instrumentalness + liveness + loudness + mode + 
                      speechiness + tempo + key + instrumentalness*danceability + 
                      duration_ms*acousticness + duration_ms*valence + duration_ms*valence + danceability*instrumentalness, 
                    data = eighties_train, family = "binomial")
summary(eighties_log)

eighties_pred_log <- predict(eighties_log, eighties_test, type = "response")
eighties_pred_log <- ifelse(eighties_pred_log > .23, 1, 0)
summary(eighties_pred_log)
confusionMatrix(as.factor(eighties_pred_log), as.factor(eighties_test$eighties))
```
First we constructed a Logistic Regression to predict whether a song was released in the 80s. The regression resulted in an accuracy of 83.93% and a Kappa value of .2623. The model has many false negatives and false positives and doesn't seem to do a great job distinguishing between 80s music and other decades. Our model suggests that 80s songs have less acousticness, more instrumentals, are more danceable, and have high energy. They also tend to have a faster tempo.

One qualifying factor is that music progression and change isn't discrete even though we are trying to classify music into discrete buckets. For example, a song released in 1979 is likely to sound like a song released in 1980 despite being from a different decade. Additionally, I made the threshold for the log odds value needed to be classified as a 80s song .23 because it maximized the Kappa value for the model. The lower threshold value makes sense because any given song is much more likely to not be from the 80s, so our model has trouble indicating with higher certainty that a song is from the 80s.

```{r}

library(C50)
error_costs = matrix(c(0, 1, 2, 0), nrow = 2)
eighties_dtm <- C5.0(as.factor(eighties) ~ acousticness + danceability + duration_ms + energy + explicit + 
                      instrumentalness + liveness + loudness + mode + 
                      speechiness + tempo + key, data = eighties_train, costs = error_costs)


eighties_pred_dt <- predict(eighties_dtm, eighties_test)
confusionMatrix(as.factor(eighties_pred_dt), as.factor(eighties_test$eighties))

```
Our decision tree model performed slightly better than our logistc regression model. The decision tree has an accuracy of 85.69% and a Kappa value of .2643. Because there are many more non 80s songs than 80s, I added a cost matrix to encouraget the decision tree to minimize false negative responses.

## Which decades are easiest to predict?
```{r}
decade_dtm <- C5.0(decades ~ acousticness + danceability + duration_ms + energy + explicit + 
                      instrumentalness + liveness + loudness + 
                      speechiness + tempo + valence, data = spotify_train)

decades_pred <- predict(decade_dtm, spotify_test, type = "class")
summary(decades_pred)
confusionMatrix(decades_pred, as.factor(spotify_test$decades))
```
The decision tree performed decently well considering how many classes there are and the difficulties in predcting a song's release decade. The model has an accuracy of 34.75% and a Kappa value of .2724. Its interesting to look along the diagnol in the confusion matrix and see that for any given decade of music, the decision tree model predicted the correct decade more than any other individual decade. This indicated that the model is catching onto the trends and differences in music by decade. 

It's also interesting to look at how the decision tree is more likely to mistake a songs decade with an adjacent decade than one further away in time. 
```{r}
fifties_preds <- c(124, 298, 598, 1121, 579, 236, 131, 153, 63, 153, 19)

barplot(fifties_preds, main="Decision Tree Predictions for Songs made in the 50s",
        xlab="Predicted Decade", names.arg=c("20s", "30s", "40s", "50s", "60s", "70s", "80s", "90s", "2000s", "2010s", "2020s"))
```

For example, for all 50s songs, our model accurately predicted 1121 of them. One decade away from the 50s is the 40s and 60s, which recieved 598 and 579 predictions respectively. The general trend is the further away in time a decade is from a songs true decade, the less likely our model is to incorrectly classify it into that decade. this relates back to my comments above about change in music not being discrete but continuous. Songs from the 50s and 60s will sound  more alike than songs from the 50s and 90s, and our model is picking up on that. There are many similarities between the musical decades, and music waves and changes don't necessarily line up with the end of a decade and the start of a new one. That's why its interestign to see and interpret our models results in a histogram rather than just as a binary correct or incorrect prediction.

# Danceability Prediction

__What is danceability?__
Spotify defines danceability as follows:
``Danceability describes how suitable a track is for dancing based on a combination of musical elements including tempo, rhythm stability, beat strength, and overall regularity.
A value of 0.0 is least danceable and 1.0 is most danceable.''

## Description of Data and Procedure

In this analysis we will run linear running regression models on the audio features train data to predict to what extent the danceability algorithm incorporates various elements of musicality, including those self-reported in Spotify's danceability definition, as well as other objective and subjective attributes available to our analysis.

### Limitations

It is important to note that there is an inherent limitation in predictive power when subjective variables are used due to bias and reliance on imperfect algorithms. That is, there is no human music assessor who is individually ranking Spotify's wealth of streamable music for perceived `danceability`. Rather, `danceability` as a variable in the Spotify API represents Spotify developers' algorithm's best attempt at predicting danceability based on the criteria which they consider most important to making music suitable for dancing, such as tempo and beat regularity. The algorithm does not necessarily output the same `danceability` rating had the developers individually listened to the track and assigned it a danceability score.
\n
Therefore, while the concept of measuring subjective attributes such as danceability is intellectually attractive for use cases such as allowing users to filter music, improving recommendation algorithms, and pre-populating automated playlists, these measurements are imperfect. For instance [this Quora post](https://www.quora.com/How-does-Spotify-calculate-the-danceability-of-a-song) written by pianist Juan Maria Solare links to [this dreamy minimal piano solo](https://play.spotify.com/track/2I97NDZU4o16A1nP9zgMah) which may not be easily danceable for most audiences, yet is ranked by Spotify as a 6.7/10 for `danceability`, sitting at the 71st percentile within the `danceability` distribution:
```{r}
#Look at danceability distribution in `audio_features`
boxplot(audio_features$danceability, horizontal=T, main="Danceability Distribution")
quantile(audio_features$danceability, probs = c(.7,.71,.72,.75))   #Score of 6.7 falls at ~71st percentile
```


### Train and Test Sets for Tracks
Use same test indexes for the track dataset
__Train & Test Sets for the Danceability Prediction__
```{r}
#audio_features
audio_train <- audio_features[-test_set,]
audio_test <- audio_features[test_set,]
#Response / Labels
audio_train_labs <- audio_features[-test_set,"danceability"]
audio_test_labs <- audio_features[test_set,"danceability"]
```
## Linear Danceability Prediction
```{r dm1}
dm1 <- lm(danceability ~ ., data = audio_train)
summary(dm1)
```
The adjusted r^2 is only .45, even though most of the explanatory variables appear to be highly significant. To get a better sense of the high error residuals, we can plot some residuals after removing any insignificant variables with the `step()` function.


```{r dance_step, cache=T}
step_dm1 = step(dm1, type = "both") 
```

The `step` function leaves the following significant relation:

`step_dm1`:
danceability ~ duration_ms + explicit + energy + key0 + key1 + 
    key2 + key3 + key4 + key5 + key7 + key9 + key10 + loudness + 
    mode + speechiness + acousticness + instrumentalness + liveness + 
    valence + tempo + time_signature + release_year
    


How well does `step_dm1` predict danceability score?


```{r}

# Apply   step_dm1` to test set
step_dm1_results <- predict(step_dm1,newdata = audio_test, type="response")

# Calculate errors between danceability predictions and test labels
step_dm1_error <- audio_test_labs - step_dm1_results
step_dm1_test_RMSE <- sqrt(mean(step_dm1_error^2))  # RSME = 0.126
step_dm1_test_r2 <- R2(step_dm1_results,audio_test_labs,form="traditional")  #R^2 = 0.456
step_dm1_test_MAE <- MAE(step_dm1_results,audio_test_labs) # MAE=0.101
step_dm1_test_MSE <- mean((step_dm1_error)^2) #MSE = 0.016

step_dm1_error_table <- matrix(c(step_dm1_test_MSE,step_dm1_test_MAE,step_dm1_test_RMSE,step_dm1_test_r2),ncol=4,byrow=T)
colnames(step_dm1_error_table) <- c("MSE", "MAE", "RSME", "R-squared")
rownames(step_dm1_error_table) <- c("step_dm1")
step_dm1_error_table <- as.table(step_dm1_error_table)

step_dm1_error_table

barplot(step_dm1_error_table[,c(1:3)],legend=F,beside=T,main='Summary Statistics for step_dm1 Prediction')   #Make legend=T after adding more regressions
```
__Interpret the error metrics__


If we recall, Spotify included music elements such as musical elements including tempo, rhythm stability, beat strength, and overall regularity. Therefore, we would expect a very high correlation between corresponding available explanatory variables, such as `tempo`. Let's quantify the correlation between `tempo` and `danceability`.
```{r}
dm2 <- ggplot(data=audio_train, aes(x=tempo, y=danceability)) + geom_point()
dm2
```
This chart takes a very interesting appearance, with a clear quadratic interaction (as danceability peaks around medium-tempo). However the data has a large variance.



plotting residuals

#Consider interaction terms
ex. speechiness & valence, tempo quadratic term



# Popularity Prediction
## What Makes a Song Popular?
Though the artist or record label may be a driving factor in the popularity of a song, predicting popularity based on song attributes alone could be very valuable information for an artist, someone who works for an artist, or a music streaming service. To find the tendencies of the most popular songs and try to see if we can classify them, we broke the data set into 2 classes: "mega-hit", a song in the top 10% of popularity on Spotify, and all other songs. Popularity of songs is never guaranteed even with multi-million dollar marketing budgets for popular artists, so any insight into what interior attributes make a song popular can be a strategic advantage. 

### Log Model for Hit Classification (mega-hit vs not) 
We can start by training a logistic regression model to classify songs as mega-hits vs non-mega-hits.
```{r, cache=TRUE}
log_test_labels <- spotify_test$binary_hit_class
log_test <- subset(spotify_test, select=-c(binary_hit_class))
log_train <- subset(spotify_train, select=-c(artists, popularity, year, hit_class))
log_model <- train(binary_hit_class ~ ., data = log_train, method = "glm", family="binomial")
summary(log_model)
```
The coefficients and p-values tell us that songs that are highly danceable, not too long and have lower energy, speechiness and liveness. We can use predict and the model generated by the train function to make a classification for our test set.
#### Evaluate Log Model
```{r}
pred <- predict(log_model, log_test)
confusionMatrix(pred, log_test_labels)
```
The logistic model attained an overall accuracy of ~91%, though given the vast majority of the elements in the test set were not hits, the overall accuracy is less descriptive than we'd like. Though classifying a song as not a mega-hit is still useful, our main prediction of interest is to find which songs will be mega-hits. Of the 1057 songs that our model classified as a mega-hit, it successfully classified 721 of them (~68%).
### KNN
Another good option specifically for classifying human behavioral tendencies (popular culture in this case) is a K-nearest neighbors algorithm. To predict a new song's popularity, the algorithm will search for songs with similar attributes and make a classification based on the popularity of those nearby songs. We normalize this data to be on a 0 to 1 scale so as not to overemphasize inputs with larger magnitude. 


```{r}
# create function to normalize dataset for KNN
normalize <- function(x){
  return ((x-min(x))/(max(x)-min(x)))
}
```

```{r}
knn_train <- subset(log_train, select=-c(decades, key))
knn_test <- subset(log_test, select=-c(artists, popularity, year, decades, hit_class, key))
knn_train <- as.data.frame(lapply(knn_train[1:(ncol(knn_train)-1)], normalize))
knn_test <- as.data.frame(lapply(knn_test[1:ncol(knn_test)], normalize))
knn_model_pred <- knn(train=knn_train, test=knn_test, cl=log_train$binary_hit_class, k=95)
```

#### Evaluate KNN Model
```{r}
confusionMatrix(knn_model_pred, log_test_labels)
```

The KNN model also faired decently well in classifying "mega-hit" songs, having a slightly lower accuracy than the log model at ~90%. Additionally the KNN was less willing to make a mega-hit classification, which is reasonable due to the significantly higher amount of non hits in the data set. The KNN still correctly classified 458/692 "mega-hit" classifications, about 66%. The model improved slightly by dropping K, the threshold for making a classification, to 1/4 of the square root of input rows, which is the typical starting point for a K value. Overall, the logistic model was more successful in completing the task we set out to accomplish in building these models: correctly predict which songs have the potential to become extremely popular. 
## Holding artist constant, what makes a song popular?

# Conclusion
